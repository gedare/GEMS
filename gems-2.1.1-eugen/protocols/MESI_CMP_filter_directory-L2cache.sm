
/*
    Copyright (C) 1999-2005 by Mark D. Hill and David A. Wood for the
    Wisconsin Multifacet Project.  Contact: gems@cs.wisc.edu
    http://www.cs.wisc.edu/gems/

    --------------------------------------------------------------------

    This file is part of the SLICC (Specification Language for
    Implementing Cache Coherence), a component of the Multifacet GEMS
    (General Execution-driven Multiprocessor Simulator) software
    toolset originally developed at the University of Wisconsin-Madison.
                                                                                
    SLICC was originally developed by Milo Martin with substantial
    contributions from Daniel Sorin.

    Substantial further development of Multifacet GEMS at the
    University of Wisconsin was performed by Alaa Alameldeen, Brad
    Beckmann, Jayaram Bobba, Ross Dickson, Dan Gibson, Pacia Harper,
    Derek Hower, Milo Martin, Michael Marty, Carl Mauer, Michelle Moravan,
    Kevin Moore, Manoj Plakal, Daniel Sorin, Haris Volos, Min Xu, and Luke Yen.

    --------------------------------------------------------------------

    If your use of this software contributes to a published paper, we
    request that you (1) cite our summary paper that appears on our
    website (http://www.cs.wisc.edu/gems/) and (2) e-mail a citation
    for your published paper to gems@cs.wisc.edu.

    If you redistribute derivatives of this software, we request that
    you notify us and either (1) ask people to register with us at our
    website (http://www.cs.wisc.edu/gems/) or (2) collect registration
    information and periodically send it to us.

    --------------------------------------------------------------------

    Multifacet GEMS is free software; you can redistribute it and/or
    modify it under the terms of version 2 of the GNU General Public
    License as published by the Free Software Foundation.

    Multifacet GEMS is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
    General Public License for more details.

    You should have received a copy of the GNU General Public License
    along with the Multifacet GEMS; if not, write to the Free Software
    Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
    02111-1307, USA

    The GNU General Public License is contained in the file LICENSE.

### END HEADER ###
*/
/*
 * $Id: MSI_MOSI_CMP_directory-L2cache.sm 1.12 05/01/19 15:55:40-06:00 beckmann@s0-28.cs.wisc.edu $
 *
 */

machine(L2Cache, "MESI Directory L2 Cache CMP") {

  // L2 BANK QUEUES
  // From local bank of L2 cache TO the network
  MessageBuffer DirRequestFromL2Cache, network="To", virtual_network="2", ordered="false";  // this L2 bank -> Memory
  MessageBuffer L1RequestFromL2Cache, network="To", virtual_network="1", ordered="false";  // this L2 bank -> a local L1
  MessageBuffer responseFromL2Cache, network="To", virtual_network="3", ordered="false";  // this L2 bank -> a local L1 || Memory

  // FROM the network to this local bank of L2 cache
  MessageBuffer L1RequestToL2Cache, network="From", virtual_network="0", ordered="false";  // a local L1 -> this L2 bank
  MessageBuffer responseToL2Cache, network="From", virtual_network="3", ordered="false";  // a local L1 || Memory -> this L2 bank
  MessageBuffer unblockToL2Cache, network="From", virtual_network="4", ordered="false";  // a local L1 || Memory -> this L2 bank

  // STATES
  enumeration(State, desc="L2 Cache states", default="L2Cache_State_NP") {
    // Base states
    NP, desc="Not present in either cache";
    SS, desc="L2 cache entry Shared, also present in one or more L1s";
    M, desc="L2 cache entry Modified, not present in any L1s", format="!b";
    MT, desc="L2 cache entry Modified in a local L1, assume L2 copy stale", format="!b";

    // L2 replacement
    M_I, desc="L2 cache replacing, have all acks, sent dirty data to memory, waiting for ACK from memory";
    MT_I, desc="L2 cache replacing, getting data from exclusive";
    MCT_I, desc="L2 cache replacing, clean in L2, getting data or ack from exclusive";
    I_I, desc="L2 replacing clean data, need to inv sharers and then drop data";
    S_I, desc="L2 replacing dirty data, collecting acks from L1s";

    // Transient States for fetching data from memory
    ISS, desc="L2 idle, got single L1_GETS, issued memory fetch, have not seen response yet";
    IS, desc="L2 idle, got L1_GET_INSTR or multiple L1_GETS, issued memory fetch, have not seen response yet";
    IM, desc="L2 idle, got L1_GETX, issued memory fetch, have not seen response(s) yet";

    // Blocking states 
    SS_MB, desc="Blocked for L1_GETX from SS";
    SS_SSB, desc="Blocked for L1_GETS from SS";
    MT_MB, desc="Blocked for L1_GETX from MT";
    M_MB, desc="Blocked for L1_GETX from M";
    ISS_MB, desc="Blocked for L1_GETS or L1_GETX from NP, received Mem Data";
    IS_SSB, desc="Blocked for L1_GET_INSTR from NP, received Mem Data";
    M_SSB, desc="Blocked for L1_GET_INSTR from M";

    MT_IIB, desc="Blocked for L1_GETS from MT, waiting for unblock and data";
    MT_IB, desc="Blocked for L1_GETS from MT, got unblock, waiting for data";
    MT_SB, desc="Blocked for L1_GETS from MT, got data,  waiting for unblock";

    // for resolving PUTX/PUTS races
    PB_MT, desc="Going to MT, got data and unblock, waiting for PUT";
    PB_SS, desc="Going to SS, got unblock, waiting for PUT";
    PB_MT_IB, desc="Blocked from MT, got unblock, waiting for data and PUT";

  }

  // EVENTS
  enumeration(Event, desc="L2 Cache events") {
    // L2 events

    // events initiated by the local L1s
    L1_GET_INSTR,            desc="a L1I GET INSTR request for a block maped to us";
    L1_GET_INSTR_ESCAPE,     desc="a L1I GET INSTR in an escape action request for a block mapped to us";
    L1_GETS,                 desc="a L1D GETS request for a block maped to us";
    L1_GETS_ESCAPE,          desc="a L1D GETS in an escape action request for a block mapped to us";
    L1_GETX,                 desc="a L1D GETX request for a block maped to us";
    L1_GETX_ESCAPE,          desc="a L1D GETX in an escape action request for a block mapped to us";
    L1_UPGRADE,                 desc="a L1D GETX request for a block maped to us";

    L1_PUTX,                 desc="L1 replacing data";
    L1_PUTX_old,             desc="L1 replacing data, but no longer sharer";
    L1_PUTS,                 desc="L1 replacing clean data";
    L1_PUTS_old,             desc="L1 replacing clean data, but no longer sharer";
    L1_PUT_PENDING,          desc="L1 PUT msg pending (recycled)"; 
 
    Fwd_L1_GETX,             desc="L1 did not have data, so we supply";
    Fwd_L1_GETS,             desc="L1 did not have data, so we supply";
    Fwd_L1_GET_INSTR,             desc="L1 did not have data, so we supply";

    // events initiated by this L2
    L2_Replacement,     desc="L2 Replacement", format="!r";
    L2_Replacement_XACT,     desc="L2 Replacement of trans. data", format="!r";
    L2_Replacement_clean,     desc="L2 Replacement, but data is clean", format="!r";
    L2_Replacement_clean_XACT,     desc="L2 Replacement of trans. data, but data is clean", format="!r";

    // events from memory controller
    Mem_Data,     desc="data from memory", format="!r";
    Mem_Ack,     desc="ack from memory", format="!r";

    // M->S data writeback
    WB_Data,  desc="data from L1";
    WB_Data_clean,  desc="clean data from L1";
    Ack,      desc="writeback ack";
    Ack_all,      desc="writeback ack";
    // For transactional memory
    Nack,      desc="filter indicates conflict";
    Nack_all,  desc="all filters have responded, at least one conflict";

    Unblock, desc="Unblock from L1 requestor";
    Unblock_Cancel, desc="Unblock from L1 requestor (FOR XACT MEMORY)";
    Exclusive_Unblock, desc="Unblock from L1 requestor";

    Unblock_WaitPUTold, desc="Unblock from L1 requestor, last requestor was replacing so wait for PUT msg";
    Exclusive_Unblock_WaitPUTold, desc="Unblock from L1 requestor, last requestor was replacing so wait for PUT msg";

  }

  // TYPES

  // CacheEntry
  structure(Entry, desc="...", interface="AbstractCacheEntry") {
    State CacheState,          desc="cache state";
    NetDest Sharers,           desc="tracks the L1 shares on-chip";
    MachineID Exclusive,       desc="Exclusive holder of block";
    DataBlock DataBlk,         desc="data for the block";
    bool Dirty, default="false", desc="data is dirty";

    bool Trans,              desc="dummy bit for debugging";
    bool Read,               desc="LogTM R bit";
    bool Write,              desc="LogTM W bit";
    bool L2Miss,             desc="Was this block sourced from memory";
    int L1PutsPending, default="0", desc="how many  PUT_ are pending for this entry (being recyled)";
  }

  // TBE fields
  structure(TBE, desc="...") {
    Address Address,            desc="Line address for this TBE";
    Address PhysicalAddress,    desc="Physical address for this TBE";
    State TBEState,             desc="Transient state";
    DataBlock DataBlk,          desc="Buffer for the data block";
    bool Dirty, default="false", desc="Data is Dirty";

    NetDest L1_GetS_IDs,            desc="Set of the internal processors that want the block in shared state";
    MachineID L1_GetX_ID,          desc="ID of the L1 cache to forward the block to once we get a response";
    bool isPrefetch,            desc="Set if this was caused by a prefetch";

    int pendingAcks,            desc="number of pending acks for invalidates during writeback";
    bool nack,  default="false",  desc="has this request been NACKed?";
  }

  external_type(CacheMemory) {
    bool cacheAvail(Address);
    Address cacheProbe(Address);
    void allocate(Address);
    void deallocate(Address);
    Entry lookup(Address);
    void changePermission(Address, AccessPermission);
    bool isTagPresent(Address);
    void setMRU(Address);
  }

  external_type(TBETable) {
    TBE lookup(Address);
    void allocate(Address);
    void deallocate(Address);
    bool isPresent(Address);
  }

  TBETable L2_TBEs, template_hack="<L2Cache_TBE>";

  CacheMemory L2cacheMemory, template_hack="<L2Cache_Entry>", constructor_hack='L2_CACHE_NUM_SETS_BITS,L2_CACHE_ASSOC,MachineType_L2Cache,int_to_string(i)';

  // inclusive cache, returns L2 entries only
  Entry getL2CacheEntry(Address addr), return_by_ref="yes" {
    return L2cacheMemory[addr];
  }
  
  void changeL2Permission(Address addr, AccessPermission permission) {
    if (L2cacheMemory.isTagPresent(addr)) {
      return L2cacheMemory.changePermission(addr, permission);
    }
  }

  string getCoherenceRequestTypeStr(CoherenceRequestType type) {
    return CoherenceRequestType_to_string(type);
  }

  bool isL2CacheTagPresent(Address addr) {
    return (L2cacheMemory.isTagPresent(addr));
  }

  bool isOneSharerLeft(Address addr, MachineID requestor) {
    assert(L2cacheMemory[addr].Sharers.isElement(requestor));
    return (L2cacheMemory[addr].Sharers.count() == 1);
  }

  bool isSharer(Address addr, MachineID requestor) {
    if (L2cacheMemory.isTagPresent(addr)) {
      return L2cacheMemory[addr].Sharers.isElement(requestor);
    } else {
      return false;
    }
  }

  void addSharer(Address addr, MachineID requestor) {
    DEBUG_EXPR(machineID);
    DEBUG_EXPR(requestor);
    DEBUG_EXPR(addr);
    assert(map_L1CacheMachId_to_L2Cache(addr, requestor) == machineID);
    L2cacheMemory[addr].Sharers.add(requestor);
  }

  State getState(Address addr) {
    if(L2_TBEs.isPresent(addr)) { 
      return L2_TBEs[addr].TBEState;
    } else if (isL2CacheTagPresent(addr)) {
      return getL2CacheEntry(addr).CacheState;
    }
    return State:NP;
  }

  string getStateStr(Address addr) {
    return L2Cache_State_to_string(getState(addr));
  }

  // when is this called
  void setState(Address addr, State state) {

    // MUST CHANGE
    if (L2_TBEs.isPresent(addr)) {
      L2_TBEs[addr].TBEState := state;
    }

    if (isL2CacheTagPresent(addr)) {
      getL2CacheEntry(addr).CacheState := state;
    
      // Set permission  
      if (state == State:SS ) {
        changeL2Permission(addr, AccessPermission:Read_Only);
      } else if (state == State:M) {
        changeL2Permission(addr, AccessPermission:Read_Write);
      } else if (state == State:MT) {
        changeL2Permission(addr, AccessPermission:Stale);
      } else { 
        changeL2Permission(addr, AccessPermission:Busy);
      }
    }
  }

  Event L1Cache_request_type_to_event(CoherenceRequestType type, Address addr, MachineID requestor) {
    if (L2cacheMemory.isTagPresent(addr)){ /* Present */
      if(getL2CacheEntry(addr).L1PutsPending > 0 &&  /* At least one PUT pending */
         (getL2CacheEntry(addr).CacheState == State:SS || getL2CacheEntry(addr).CacheState == State:MT || getL2CacheEntry(addr).CacheState == State:M )) {  /* Base state */

        /* Only allow PUTX/PUTS to go on  */
        if (type != CoherenceRequestType:PUTX &&
            type != CoherenceRequestType:PUTS)  {
          return Event:L1_PUT_PENDING;       // Don't serve any req until the wb is serviced
        }
      }
    }
    if(type == CoherenceRequestType:GETS) {
      return Event:L1_GETS;
    } else if(type == CoherenceRequestType:GETS_ESCAPE) {
      return Event:L1_GETS_ESCAPE;
    } else if(type == CoherenceRequestType:GET_INSTR) {
      return Event:L1_GET_INSTR;
    } else if(type == CoherenceRequestType:GET_INSTR_ESCAPE) {
      return Event:L1_GET_INSTR_ESCAPE;
    } else if (type == CoherenceRequestType:GETX) {
      return Event:L1_GETX;
    } else if(type == CoherenceRequestType:GETX_ESCAPE) {
      return Event:L1_GETX_ESCAPE;
    } else if (type == CoherenceRequestType:UPGRADE) {
      if ( isL2CacheTagPresent(addr) && getL2CacheEntry(addr).Sharers.isElement(requestor) ) {
        return Event:L1_UPGRADE;
      } else {
        return Event:L1_GETX;
      }
    } else if (type == CoherenceRequestType:PUTX) {
      if ( isL2CacheTagPresent(addr) && getL2CacheEntry(addr).L1PutsPending > 0) {
         getL2CacheEntry(addr).L1PutsPending := getL2CacheEntry(addr).L1PutsPending  - 1;
         DEBUG_EXPR("PUTX PutSPending ");
         DEBUG_EXPR(getL2CacheEntry(addr).L1PutsPending);
      }
      if (isSharer(addr, requestor)) {
        return Event:L1_PUTX;
      } else {
        return Event:L1_PUTX_old;
      }
    } else if (type == CoherenceRequestType:PUTS) {
      if ( isL2CacheTagPresent(addr) && getL2CacheEntry(addr).L1PutsPending > 0) {
        getL2CacheEntry(addr).L1PutsPending := getL2CacheEntry(addr).L1PutsPending  - 1;
        DEBUG_EXPR("PUTS PutSPending ");
        DEBUG_EXPR(getL2CacheEntry(addr).L1PutsPending);
      }
      if (isSharer(addr, requestor)) {
        return Event:L1_PUTS;
      } else {
        return Event:L1_PUTS_old;
      }
    } else {
      DEBUG_EXPR(addr);
      DEBUG_EXPR(type);
      error("Invalid L1 forwarded request type");
    }
  }

  // ** OUT_PORTS **

  out_port(L1RequestIntraChipL2Network_out, RequestMsg, L1RequestFromL2Cache);
  out_port(DirRequestIntraChipL2Network_out, RequestMsg, DirRequestFromL2Cache);
  out_port(responseIntraChipL2Network_out, ResponseMsg, responseFromL2Cache);


  // Response IntraChip L2 Network - response msg to this particular L2 bank
  in_port(responseIntraChipL2Network_in, ResponseMsg, responseToL2Cache) {
    if (responseIntraChipL2Network_in.isReady()) {
      peek(responseIntraChipL2Network_in, ResponseMsg) {
        // test wether it's from a local L1 or an off chip source
        assert(in_msg.Destination.isElement(machineID));
        if(machineIDToMachineType(in_msg.Sender) == MachineType:L1Cache) {
          if(in_msg.Type == CoherenceResponseType:DATA) {
            if (in_msg.Dirty) {
              trigger(Event:WB_Data, in_msg.Address);
            } else {
              trigger(Event:WB_Data_clean, in_msg.Address);
            }
          } else if (in_msg.Type == CoherenceResponseType:ACK) {
            if ((L2_TBEs[in_msg.Address].pendingAcks - in_msg.AckCount) == 0) {
              // check whether any previous responses have been NACKs
              if(L2_TBEs[in_msg.Address].nack == false) {
                trigger(Event:Ack_all, in_msg.Address);
              }
              else {
                // at least one nack received
                trigger(Event:Nack_all, in_msg.Address);
              }
            } else {
              trigger(Event:Ack, in_msg.Address);
            }
            // for NACKs
          } else if (in_msg.Type == CoherenceResponseType:NACK) {
            if ((L2_TBEs[in_msg.Address].pendingAcks - in_msg.AckCount) == 0) {
              trigger(Event:Nack_all, in_msg.Address);
            } else {
              trigger(Event:Nack, in_msg.Address);
            }
          } else {
            error("unknown message type");
          }

        } else { // external message
          if(in_msg.Type == CoherenceResponseType:MEMORY_DATA) {
            trigger(Event:Mem_Data, in_msg.Address);  // L2 now has data and all off-chip acks
          } else if(in_msg.Type == CoherenceResponseType:MEMORY_ACK) {
            trigger(Event:Mem_Ack, in_msg.Address);  // L2 now has data and all off-chip acks
          } else {
            error("unknown message type");
          }
        }
      }
    }  // if not ready, do nothing
  }

  // L1 Request
  in_port(L1RequestIntraChipL2Network_in, RequestMsg, L1RequestToL2Cache) {
    if(L1RequestIntraChipL2Network_in.isReady()) {
      peek(L1RequestIntraChipL2Network_in,  RequestMsg) {
        /*
        DEBUG_EXPR(in_msg.Address);
        DEBUG_EXPR(id);
        DEBUG_EXPR(getState(in_msg.Address));
        DEBUG_EXPR(in_msg.Requestor);
        DEBUG_EXPR(in_msg.Type);
        DEBUG_EXPR(in_msg.Destination);
        */
        assert(machineIDToMachineType(in_msg.Requestor) == MachineType:L1Cache);
        assert(in_msg.Destination.isElement(machineID));
        if (L2cacheMemory.isTagPresent(in_msg.Address)) { 
          // The L2 contains the block, so proceeded with handling the request
          trigger(L1Cache_request_type_to_event(in_msg.Type, in_msg.Address, in_msg.Requestor), in_msg.Address);
        } else {
          if (L2cacheMemory.cacheAvail(in_msg.Address)) {
            // L2 does't have the line, but we have space for it in the L2
            trigger(L1Cache_request_type_to_event(in_msg.Type, in_msg.Address, in_msg.Requestor), in_msg.Address);
          } else {
            // No room in the L2, so we need to make room before handling the request
            if (L2cacheMemory[ L2cacheMemory.cacheProbe(in_msg.Address) ].Dirty ) {
              // check whether block is transactional
              if(L2cacheMemory[ L2cacheMemory.cacheProbe(in_msg.Address) ].Trans == true){
                trigger(Event:L2_Replacement_XACT, L2cacheMemory.cacheProbe(in_msg.Address));
              }
              else{
                trigger(Event:L2_Replacement, L2cacheMemory.cacheProbe(in_msg.Address));
              }
            } else {
              // check whether block is transactional
              if(L2cacheMemory[ L2cacheMemory.cacheProbe(in_msg.Address) ].Trans == true){
                trigger(Event:L2_Replacement_clean_XACT, L2cacheMemory.cacheProbe(in_msg.Address));
              }
              else{
                trigger(Event:L2_Replacement_clean, L2cacheMemory.cacheProbe(in_msg.Address));
              }
            }
          }
        }
      }
    }
  }

  in_port(L1unblockNetwork_in, ResponseMsg, unblockToL2Cache) {
    if(L1unblockNetwork_in.isReady()) {
      peek(L1unblockNetwork_in,  ResponseMsg) {
        assert(in_msg.Destination.isElement(machineID));
        if (in_msg.Type == CoherenceResponseType:EXCLUSIVE_UNBLOCK) {
          if (in_msg.RemoveLastOwnerFromDir == true) {
            if (isSharer(in_msg.Address,in_msg.LastOwnerID)) {
              trigger(Event:Exclusive_Unblock_WaitPUTold, in_msg.Address);
            }
            else  { // PUT arrived, requestor already removed from dir
              trigger(Event:Exclusive_Unblock, in_msg.Address);
            }
          }
          else {
            trigger(Event:Exclusive_Unblock, in_msg.Address);
          }
        } else if (in_msg.Type == CoherenceResponseType:UNBLOCK) {
          if (in_msg.RemoveLastOwnerFromDir == true) { 
            if (isSharer(in_msg.Address,in_msg.LastOwnerID)) {
              trigger(Event:Unblock_WaitPUTold, in_msg.Address);
            }
            else  { // PUT arrived, requestor already removed from dir
              trigger(Event:Unblock, in_msg.Address);
            }
          }
          else {
            trigger(Event:Unblock, in_msg.Address);
          }
        } else if (in_msg.Type == CoherenceResponseType:UNBLOCK_CANCEL) {
          trigger(Event:Unblock_Cancel, in_msg.Address);
        } else {
          error("unknown unblock message");
        }
      }
    }
  }

  // ACTIONS

  action(a_issueFetchToMemory, "a", desc="fetch data from memory") {
    peek(L1RequestIntraChipL2Network_in, RequestMsg) {
      enqueue(DirRequestIntraChipL2Network_out, RequestMsg, latency="L2_REQUEST_LATENCY") {
        out_msg.Address := address;
        out_msg.PhysicalAddress := in_msg.PhysicalAddress;
        out_msg.Type := CoherenceRequestType:GETS;
        out_msg.Requestor := machineID;
        out_msg.Destination.add(map_Address_to_Directory(address));
        out_msg.MessageSize := MessageSizeType:Control;
      }
    }
  }

  action(b_forwardRequestToExclusive, "b", desc="Forward request to the exclusive L1") {
    peek(L1RequestIntraChipL2Network_in, RequestMsg) {
      enqueue(L1RequestIntraChipL2Network_out, RequestMsg, latency="L2_TAG_LATENCY") {
        out_msg.Address := address;
        out_msg.PhysicalAddress := in_msg.PhysicalAddress;
        out_msg.Type := in_msg.Type;
        out_msg.Requestor := in_msg.Requestor;
        out_msg.Destination.add(L2cacheMemory[address].Exclusive);
        out_msg.MessageSize := MessageSizeType:Request_Control;
        // also pass along timestamp
        out_msg.Timestamp := in_msg.Timestamp;
        APPEND_TRANSITION_COMMENT(" ");
        APPEND_TRANSITION_COMMENT(in_msg.PhysicalAddress);
      }
    }
  }

  action(c_exclusiveReplacement, "c", desc="Send data to memory") {
    enqueue(responseIntraChipL2Network_out, ResponseMsg, latency="L2_RESPONSE_LATENCY") {
      out_msg.Address := address;
      out_msg.Type := CoherenceResponseType:MEMORY_DATA;
      out_msg.Sender := machineID;
      out_msg.Destination.add(map_Address_to_Directory(address));
      out_msg.DataBlk := getL2CacheEntry(address).DataBlk;
      out_msg.Dirty := getL2CacheEntry(address).Dirty;
      out_msg.MessageSize := MessageSizeType:Response_Data;
    }
  }

  action(ct_exclusiveReplacementFromTBE, "ct", desc="Send data to memory") {
    enqueue(responseIntraChipL2Network_out, ResponseMsg, latency="L2_RESPONSE_LATENCY") {
      out_msg.Address := address;
      out_msg.Type := CoherenceResponseType:MEMORY_DATA;
      out_msg.Sender := machineID;
      out_msg.Destination.add(map_Address_to_Directory(address));
      out_msg.DataBlk := L2_TBEs[address].DataBlk;
      out_msg.Dirty := L2_TBEs[address].Dirty;
      out_msg.MessageSize := MessageSizeType:Response_Data;
    }
  }


  //************Transactional memory actions **************
  //broadcast a write filter lookup request to all L1s except for the requestor
  action(a_checkL1WriteFiltersExceptRequestor, "wr", desc="Broadcast a Write Filter lookup request"){
    peek(L1RequestIntraChipL2Network_in, RequestMsg) {
      enqueue(L1RequestIntraChipL2Network_out, RequestMsg, latency="L2_TAG_LATENCY") {
        out_msg.Address := address;
        out_msg.PhysicalAddress := in_msg.PhysicalAddress;
        out_msg.Type := CoherenceRequestType:CHECK_WRITE_FILTER;
        // make L1s forward responses to requestor
        out_msg.Requestor := in_msg.Requestor;
        out_msg.Destination := getLocalL1IDs(machineID);
        // don't broadcast to requestor
        out_msg.Destination.remove(in_msg.Requestor);
        out_msg.MessageSize := MessageSizeType:Response_Control;
        // also pass along timestamp of requestor
        out_msg.Timestamp := in_msg.Timestamp;
        APPEND_TRANSITION_COMMENT(" ");
        APPEND_TRANSITION_COMMENT(in_msg.PhysicalAddress);
      }
    }
  }

  //broadcast a read + write filter lookup request to all L1s except for the requestor
  action(a_checkL1ReadWriteFiltersExceptRequestor, "rwr", desc="Broadcast a Read + Write Filter lookup request"){
    peek(L1RequestIntraChipL2Network_in, RequestMsg) {
      enqueue(L1RequestIntraChipL2Network_out, RequestMsg, latency="L2_TAG_LATENCY") {
        out_msg.Address := address;
        out_msg.PhysicalAddress := in_msg.PhysicalAddress;
        out_msg.Type := CoherenceRequestType:CHECK_READ_WRITE_FILTER;
        // make L1 forward responses to requestor
        out_msg.Requestor := in_msg.Requestor;
        out_msg.Destination := getLocalL1IDs(machineID);
        // don't broadcast to requestor
        out_msg.Destination.remove(in_msg.Requestor);
        out_msg.MessageSize := MessageSizeType:Response_Control;
        // also pass along timestamp of requestor
        out_msg.Timestamp := in_msg.Timestamp;
        APPEND_TRANSITION_COMMENT(" ");
        APPEND_TRANSITION_COMMENT(in_msg.PhysicalAddress);
      }
    }
  }

  // These are to send out filter checks to those NACKers in our sharers or exclusive ptr list
  action(a_checkNackerL1WriteFiltersExceptRequestor, "wrn", desc="Broadcast a Write Filter lookup request"){
    peek(L1RequestIntraChipL2Network_in, RequestMsg) {
      // check if the L2 miss bit is set - if it is, send check filter requests to those in our Sharers list only
      if(getL2CacheEntry(address).L2Miss == true){
        // check whether we are the only sharer on the list. If so, no need to broadcast. 
        if(isSharer(address, in_msg.Requestor) == true && isOneSharerLeft(address, in_msg.Requestor) == true){
          // no filter check needed
          APPEND_TRANSITION_COMMENT("L2 Miss: No need to check L1 write filter ");
          APPEND_TRANSITION_COMMENT(in_msg.Requestor);
        }
        else{
          enqueue(L1RequestIntraChipL2Network_out, RequestMsg, latency="L2_TAG_LATENCY") {
            out_msg.Address := address;
            out_msg.PhysicalAddress := in_msg.PhysicalAddress;
            out_msg.Type := CoherenceRequestType:CHECK_WRITE_FILTER;
            // make L1s forward responses to requestor
            out_msg.Requestor := in_msg.Requestor;
            assert(getL2CacheEntry(address).Sharers.count() > 0);
            out_msg.Destination := getL2CacheEntry(address).Sharers;
            // don't broadcast to requestor
            out_msg.Destination.remove(in_msg.Requestor);
            out_msg.MessageSize := MessageSizeType:Response_Control;
            // also pass along timestamp of requestor
            out_msg.Timestamp := in_msg.Timestamp;
            APPEND_TRANSITION_COMMENT(" ");
            APPEND_TRANSITION_COMMENT(in_msg.PhysicalAddress);
            APPEND_TRANSITION_COMMENT(" requestor: ");
            APPEND_TRANSITION_COMMENT(in_msg.Requestor);
            APPEND_TRANSITION_COMMENT(" dest: ");
            APPEND_TRANSITION_COMMENT(out_msg.Destination);
          }
        }
      }
      else{
        // This is a read request, so check whether we have a writer
        if(getL2CacheEntry(address).Sharers.count() == 0 && getL2CacheEntry(address).Exclusive != in_msg.Requestor){
          enqueue(L1RequestIntraChipL2Network_out, RequestMsg, latency="L2_TAG_LATENCY") {
            // we have a writer, and it is not us
            out_msg.Address := address;
            out_msg.PhysicalAddress := in_msg.PhysicalAddress;
            out_msg.Type := CoherenceRequestType:CHECK_WRITE_FILTER;
            // make L1s forward responses to requestor
            out_msg.Requestor := in_msg.Requestor;
            out_msg.Destination.add(getL2CacheEntry(address).Exclusive);
            out_msg.MessageSize := MessageSizeType:Response_Control;
            // also pass along timestamp of requestor
            out_msg.Timestamp := in_msg.Timestamp;
            APPEND_TRANSITION_COMMENT(" ");
            APPEND_TRANSITION_COMMENT(in_msg.PhysicalAddress);
            APPEND_TRANSITION_COMMENT(" requestor: ");
            APPEND_TRANSITION_COMMENT(in_msg.Requestor);
            APPEND_TRANSITION_COMMENT(" dest: ");
            APPEND_TRANSITION_COMMENT(out_msg.Destination);
          }
        }
        else{
          APPEND_TRANSITION_COMMENT("L1 replacement: No need to check L1 write filter");
          APPEND_TRANSITION_COMMENT(" requestor: ");
          APPEND_TRANSITION_COMMENT(in_msg.Requestor);
          APPEND_TRANSITION_COMMENT(" exclusive: ");
          APPEND_TRANSITION_COMMENT(getL2CacheEntry(address).Exclusive);
          // we should not have any sharers
          assert( getL2CacheEntry(address).Sharers.count() == 0 );
        }
      }
    }
  }

  action(a_checkNackerL1ReadWriteFiltersExceptRequestor, "wrrn", desc="Broadcast a Read + Write Filter lookup request"){
    peek(L1RequestIntraChipL2Network_in, RequestMsg) {
      // check if the L2 miss bit is set - if it is, send check filter requests to those in our Sharers list only
      if(getL2CacheEntry(address).L2Miss == true){
        // check whether we are the only sharer on the list. If so, no need to broadcast. 
        if(isSharer(address, in_msg.Requestor) == true && isOneSharerLeft(address, in_msg.Requestor) == true){
          // no filter check needed
          APPEND_TRANSITION_COMMENT("L2 Miss: No need to check L1 read/write filter");
          APPEND_TRANSITION_COMMENT(" requestor: ");
          APPEND_TRANSITION_COMMENT(in_msg.Requestor);
        }
        else{
          enqueue(L1RequestIntraChipL2Network_out, RequestMsg, latency="L2_TAG_LATENCY") {
            out_msg.Address := address;
            out_msg.PhysicalAddress := in_msg.PhysicalAddress;
            out_msg.Type := CoherenceRequestType:CHECK_READ_WRITE_FILTER;
            // make L1s forward responses to requestor
            out_msg.Requestor := in_msg.Requestor;
            assert(getL2CacheEntry(address).Sharers.count() > 0);
            out_msg.Destination := getL2CacheEntry(address).Sharers;
            // don't broadcast to requestor
            out_msg.Destination.remove(in_msg.Requestor);
            out_msg.MessageSize := MessageSizeType:Response_Control;
            // also pass along timestamp of requestor
            out_msg.Timestamp := in_msg.Timestamp;
            APPEND_TRANSITION_COMMENT(" ");
            APPEND_TRANSITION_COMMENT(in_msg.PhysicalAddress);
            APPEND_TRANSITION_COMMENT(" requestor: ");
            APPEND_TRANSITION_COMMENT(in_msg.Requestor);
            APPEND_TRANSITION_COMMENT(" dest: ");
            APPEND_TRANSITION_COMMENT(out_msg.Destination);
          }
        }
      }
      else{
        // This is a write request, so check whether we have readers not including us or a writer that is not us
        if(getL2CacheEntry(address).Sharers.count() == 0 && getL2CacheEntry(address).Exclusive != in_msg.Requestor){
          enqueue(L1RequestIntraChipL2Network_out, RequestMsg, latency="L2_TAG_LATENCY") {
            // we have a writer, and it is not us
            out_msg.Address := address;
            out_msg.PhysicalAddress := in_msg.PhysicalAddress;
            out_msg.Type := CoherenceRequestType:CHECK_READ_WRITE_FILTER;
            // make L1s forward responses to requestor
            out_msg.Requestor := in_msg.Requestor;
            out_msg.Destination.add(getL2CacheEntry(address).Exclusive);
            out_msg.MessageSize := MessageSizeType:Response_Control;
            // also pass along timestamp of requestor
            out_msg.Timestamp := in_msg.Timestamp;
            APPEND_TRANSITION_COMMENT(" ");
            APPEND_TRANSITION_COMMENT(in_msg.PhysicalAddress);
            APPEND_TRANSITION_COMMENT(" requestor: ");
            APPEND_TRANSITION_COMMENT(in_msg.Requestor);
            APPEND_TRANSITION_COMMENT(" dest: ");
            APPEND_TRANSITION_COMMENT(out_msg.Destination);
          }
        }
        else if(getL2CacheEntry(address).Sharers.count() > 0){
          // this should never happen - since we allow silent S replacements but we always track exclusive L1
          assert(false);
          if(isSharer(address, in_msg.Requestor) == true && isOneSharerLeft(address, in_msg.Requestor) == true){
            // no filter check needed
            APPEND_TRANSITION_COMMENT(in_msg.Requestor);
            APPEND_TRANSITION_COMMENT(" L1 replacement: No need to check L1 read/write filter - we are only reader");
          }
          else{
            // reader(s) exist that is not us
            enqueue(L1RequestIntraChipL2Network_out, RequestMsg, latency="L2_TAG_LATENCY") {
              out_msg.Address := address;
              out_msg.PhysicalAddress := in_msg.PhysicalAddress;
              out_msg.Type := CoherenceRequestType:CHECK_READ_WRITE_FILTER;
              // make L1s forward responses to requestor
              out_msg.Requestor := in_msg.Requestor;
              out_msg.Destination := getL2CacheEntry(address).Sharers;
              // don't check our own filter
              out_msg.Destination.remove(in_msg.Requestor);
              out_msg.MessageSize := MessageSizeType:Response_Control;
              // also pass along timestamp of requestor
              out_msg.Timestamp := in_msg.Timestamp;
              APPEND_TRANSITION_COMMENT(" ");
              APPEND_TRANSITION_COMMENT(in_msg.PhysicalAddress);
              APPEND_TRANSITION_COMMENT(" requestor: ");
              APPEND_TRANSITION_COMMENT(in_msg.Requestor);
              APPEND_TRANSITION_COMMENT(" dest: ");
              APPEND_TRANSITION_COMMENT(out_msg.Destination);
            }
          }
        }
        else{
          APPEND_TRANSITION_COMMENT(in_msg.Requestor);
          APPEND_TRANSITION_COMMENT(" ");
          APPEND_TRANSITION_COMMENT(getL2CacheEntry(address).Sharers);
          APPEND_TRANSITION_COMMENT(" ");
          APPEND_TRANSITION_COMMENT(getL2CacheEntry(address).Exclusive);
          APPEND_TRANSITION_COMMENT(" L1 replacement: No need to check L1 read/write filter");
        }
      }
    }
  }
  
  // send data but force L1 requestor to wait for filter responses
  action(f_sendDataToGetSRequestor, "f", desc="Send data from cache to reqeustor") {
    enqueue(responseIntraChipL2Network_out, ResponseMsg, latency="L2_RESPONSE_LATENCY") {
      out_msg.Address := address;
      out_msg.PhysicalAddress := L2_TBEs[address].PhysicalAddress;
      out_msg.Type := CoherenceResponseType:L2_DATA;
      out_msg.Sender := machineID;
      out_msg.Destination := L2_TBEs[address].L1_GetS_IDs;  // internal nodes
      out_msg.DataBlk := getL2CacheEntry(address).DataBlk;
      out_msg.Dirty := getL2CacheEntry(address).Dirty;
      out_msg.MessageSize := MessageSizeType:Response_Data;
 
      // wait for the filter responses from other L1s
      out_msg.AckCount := 0 - (numberOfL1CachePerChip() - 1);
    }
  }

  // send exclusive data
  action(f_sendExclusiveDataToGetSRequestor, "fx", desc="Send data from cache to reqeustor") {
    enqueue(responseIntraChipL2Network_out, ResponseMsg, latency="L2_RESPONSE_LATENCY") {
      out_msg.Address := address;
      out_msg.PhysicalAddress := L2_TBEs[address].PhysicalAddress;
      out_msg.Type := CoherenceResponseType:L2_DATA_EXCLUSIVE;
      out_msg.Sender := machineID;
      out_msg.Destination := L2_TBEs[address].L1_GetS_IDs;  // internal nodes
      out_msg.DataBlk := getL2CacheEntry(address).DataBlk;
      out_msg.Dirty := getL2CacheEntry(address).Dirty;
      out_msg.MessageSize := MessageSizeType:Response_Data;
 
      // wait for the filter responses from other L1s
      out_msg.AckCount := 0 - (numberOfL1CachePerChip() - 1);
    }
  }

  action(f_sendDataToGetXRequestor, "fxx", desc="Send data from cache to reqeustor") {
    enqueue(responseIntraChipL2Network_out, ResponseMsg, latency="L2_RESPONSE_LATENCY") {
      out_msg.Address := address;
      out_msg.PhysicalAddress := L2_TBEs[address].PhysicalAddress;
      out_msg.Type := CoherenceResponseType:L2_DATA;
      out_msg.Sender := machineID;
      out_msg.Destination.add(L2_TBEs[address].L1_GetX_ID);  // internal nodes
      out_msg.DataBlk := getL2CacheEntry(address).DataBlk;
      out_msg.Dirty := getL2CacheEntry(address).Dirty;
      out_msg.MessageSize := MessageSizeType:Response_Data;
 
      // wait for the filter responses from other L1s
      out_msg.AckCount := 0 - (numberOfL1CachePerChip() - 1);
    }
  }

  action(f_sendDataToRequestor, "fd", desc="Send data from cache to reqeustor") {
    peek(L1RequestIntraChipL2Network_in, RequestMsg) {
      enqueue(responseIntraChipL2Network_out, ResponseMsg, latency="L2_RESPONSE_LATENCY") {
        out_msg.Address := address;
        out_msg.PhysicalAddress := in_msg.PhysicalAddress;
        out_msg.Type := CoherenceResponseType:L2_DATA;
        out_msg.Sender := machineID;
        out_msg.Destination.add(in_msg.Requestor);
        out_msg.DataBlk := getL2CacheEntry(address).DataBlk;
        out_msg.Dirty := getL2CacheEntry(address).Dirty;
        out_msg.MessageSize := MessageSizeType:Response_Data;
 
        // different ack counts for different situations
        if(in_msg.Type == CoherenceRequestType:GET_INSTR_ESCAPE || in_msg.Type == CoherenceRequestType:GETX_ESCAPE){
          // no acks needed
          out_msg.AckCount := 0;
        }
        else{

          // ORIGINAL
          if( false ) {
            out_msg.AckCount := 0 - (numberOfL1CachePerChip() - 1);
          }

          else{
            // NEW***
          // differentiate btw read and write requests
          if(in_msg.Type == CoherenceRequestType:GET_INSTR){
            if(getL2CacheEntry(address).L2Miss == true){
              // check whether we are the only sharer on the list. If so, no need to broadcast. 
              if(isSharer(address, in_msg.Requestor) == true && isOneSharerLeft(address, in_msg.Requestor) == true){
                // no filter check needed
                APPEND_TRANSITION_COMMENT("We are only sharer");
                out_msg.AckCount := 0;
              }
              else{
                // wait for ACKs from the other NACKers
                out_msg.AckCount := 0 - getL2CacheEntry(address).Sharers.count();
                if(isSharer(address, in_msg.Requestor)){
                  // don't include us
                  out_msg.AckCount := out_msg.AckCount + 1;
                }
                APPEND_TRANSITION_COMMENT("Nackers exist");
              }
            }
            else{
              // This is a read request, so check whether we have a writer
              if(getL2CacheEntry(address).Sharers.count() == 0 && getL2CacheEntry(address).Exclusive != in_msg.Requestor){
                // we have a writer and it is not us
                out_msg.AckCount := 0 - 1;
                
                APPEND_TRANSITION_COMMENT(" Writer exists ");
                APPEND_TRANSITION_COMMENT(getL2CacheEntry(address).Exclusive);
              }
              else{
                // we should have no sharers!
                assert(getL2CacheEntry(address).Sharers.count() == 0);
                assert(getL2CacheEntry(address).Exclusive == in_msg.Requestor);

                APPEND_TRANSITION_COMMENT(" Sharers or we are writer exist, ok to read ");
                APPEND_TRANSITION_COMMENT(" sharers: ");
                APPEND_TRANSITION_COMMENT(getL2CacheEntry(address).Sharers);
                APPEND_TRANSITION_COMMENT(" exclusive: ");
                APPEND_TRANSITION_COMMENT(getL2CacheEntry(address).Exclusive);
                out_msg.AckCount := 0;
              }
            }
          }
          else if(in_msg.Type == CoherenceRequestType:GETX){
            if(getL2CacheEntry(address).L2Miss == true){
              // check whether we are the only sharer on the list. If so, no need to broadcast. 
              if(isSharer(address, in_msg.Requestor) == true && isOneSharerLeft(address, in_msg.Requestor) == true){
                // no filter check needed
                APPEND_TRANSITION_COMMENT(" L2Miss and we are only sharer ");
                out_msg.AckCount := 0;
              }
              else{
                // nackers exist
                out_msg.AckCount := 0 - getL2CacheEntry(address).Sharers.count();
                if(isSharer(address, in_msg.Requestor)){
                  // don't include us
                  out_msg.AckCount := out_msg.AckCount + 1;
                }
                APPEND_TRANSITION_COMMENT("Nackers exist"); 
              }
            }
            else{
              // This is a write request, so check whether we have readers not including us or a writer that is not us
              if(getL2CacheEntry(address).Sharers.count() == 0 && getL2CacheEntry(address).Exclusive != in_msg.Requestor){
                // we have a writer and it is not us
                out_msg.AckCount := 0 - 1;
                
                APPEND_TRANSITION_COMMENT(" Writer exists ");
                APPEND_TRANSITION_COMMENT(getL2CacheEntry(address).Exclusive);
                
              }
              else if(getL2CacheEntry(address).Sharers.count() > 0){
                // this shouldn't be possible - we always track exclusive owner, but allow silent S replacements
                assert(false);

                if(isSharer(address, in_msg.Requestor) == true && isOneSharerLeft(address, in_msg.Requestor) == true){
                  // no filter check needed
                  APPEND_TRANSITION_COMMENT(" L1 replacement: No need to check L1 read/write filter - we are only reader");
                  out_msg.AckCount := 0;
                }
                else{
                  // reader(s) exist that is not us
                  out_msg.AckCount := 0 - getL2CacheEntry(address).Sharers.count();
                  if(isSharer(address, in_msg.Requestor)){
                    // don't include us
                    out_msg.AckCount := out_msg.AckCount + 1;
                  }
                  APPEND_TRANSITION_COMMENT(" Readers exist ");
                  APPEND_TRANSITION_COMMENT(getL2CacheEntry(address).Sharers);
                }
              }
              else{
                // we should always have no sharers!
                assert(getL2CacheEntry(address).Sharers.count() == 0);
                assert(getL2CacheEntry(address).Exclusive == in_msg.Requestor);

                out_msg.AckCount := 0;

                APPEND_TRANSITION_COMMENT(" sharers: ");
                APPEND_TRANSITION_COMMENT(getL2CacheEntry(address).Sharers);
                APPEND_TRANSITION_COMMENT(" exclusive: ");
                APPEND_TRANSITION_COMMENT(getL2CacheEntry(address).Exclusive);
                APPEND_TRANSITION_COMMENT(" L1 replacement: No need to check L1 read/write filter");
              }
            }
          } // for GETX
          else{
            // unknown request type
            APPEND_TRANSITION_COMMENT(in_msg.Type);
            APPEND_TRANSITION_COMMENT(" ");
            APPEND_TRANSITION_COMMENT(in_msg.Requestor);
            assert(false);
          }
        }
        } // for original vs new code
        APPEND_TRANSITION_COMMENT(" requestor: ");
        APPEND_TRANSITION_COMMENT(in_msg.Requestor);
        APPEND_TRANSITION_COMMENT(" AckCount: ");
        APPEND_TRANSITION_COMMENT(out_msg.AckCount);
      }
    }
  }

  action(f_sendExclusiveDataToRequestor, "fdx", desc="Send data from cache to reqeustor") {
    peek(L1RequestIntraChipL2Network_in, RequestMsg) {
      enqueue(responseIntraChipL2Network_out, ResponseMsg, latency="L2_RESPONSE_LATENCY") {
        out_msg.Address := address;
        out_msg.PhysicalAddress := in_msg.PhysicalAddress;
        out_msg.Type := CoherenceResponseType:L2_DATA_EXCLUSIVE;
        out_msg.Sender := machineID;
        out_msg.Destination.add(in_msg.Requestor);
        out_msg.DataBlk := getL2CacheEntry(address).DataBlk;
        out_msg.Dirty := getL2CacheEntry(address).Dirty;
        out_msg.MessageSize := MessageSizeType:Response_Data;
 
        // different ack counts depending on situation 
        // IMPORTANT: assuming data sent exclusively for GETS request
        if(in_msg.Type == CoherenceRequestType:GETS_ESCAPE){
          // no acks needed
          out_msg.AckCount := 0;
        }
        else{

          // ORIGINAL :
          if( false ){
            // request filter checks from all L1s
            out_msg.AckCount := 0 - (numberOfL1CachePerChip() - 1);
          }
          else{
            // NEW***
          if(getL2CacheEntry(address).L2Miss == true){
            // check whether we are the only sharer on the list. If so, no need to broadcast. 
            if(isSharer(address, in_msg.Requestor) == true && isOneSharerLeft(address, in_msg.Requestor) == true){
              // no filter check needed
              APPEND_TRANSITION_COMMENT("We are only sharer");
              out_msg.AckCount := 0;
            }
            else{
              // wait for ACKs from the other NACKers
              out_msg.AckCount := 0 - getL2CacheEntry(address).Sharers.count();
              if(isSharer(address, in_msg.Requestor)){
                // don't include us
                out_msg.AckCount := out_msg.AckCount + 1;
              }
              APPEND_TRANSITION_COMMENT("Nackers exist");
            }
          }
          else{
            // This is a read request, so check whether we have a writer
            if(getL2CacheEntry(address).Sharers.count() == 0 && getL2CacheEntry(address).Exclusive != in_msg.Requestor){
              // we have a writer and it is not us
              out_msg.AckCount := 0 - 1;
              
              APPEND_TRANSITION_COMMENT(" Writer exists ");
              APPEND_TRANSITION_COMMENT(getL2CacheEntry(address).Exclusive);
            }
            else{
              // we should always have no sharers!
              APPEND_TRANSITION_COMMENT(address);
              APPEND_TRANSITION_COMMENT(" requestor: ");
              APPEND_TRANSITION_COMMENT(in_msg.Requestor);
              APPEND_TRANSITION_COMMENT(" sharers: ");
              APPEND_TRANSITION_COMMENT(getL2CacheEntry(address).Sharers);

              DEBUG_EXPR(address);
              DEBUG_EXPR(" requestor: ");
              DEBUG_EXPR(in_msg.Requestor);
              DEBUG_EXPR(" sharers: ");
              DEBUG_EXPR(getL2CacheEntry(address).Sharers);
              
              assert(getL2CacheEntry(address).Sharers.count() == 0);
              assert(getL2CacheEntry(address).Exclusive == in_msg.Requestor);
              APPEND_TRANSITION_COMMENT(" Sharers exist or we are writer, ok to read ");
              out_msg.AckCount := 0;
            }
          }
          } // for orginal vs new code
        }
        
        APPEND_TRANSITION_COMMENT(" requestor: ");
        APPEND_TRANSITION_COMMENT(in_msg.Requestor);
        APPEND_TRANSITION_COMMENT(" AckCount: ");
        APPEND_TRANSITION_COMMENT(out_msg.AckCount);
      }
    }
  }

  // send an accumulated ACK to requestor when we don't care about checking filters (for escape actions)
  action(f_sendAccumulatedAckToRequestor, "faa", desc="Send ACKs to requestor") {
    // special case: don't send ACK if uniprocessor, since we don't need it (just send data)
    if((numberOfL1CachePerChip() - 1) > 0){
      peek(L1RequestIntraChipL2Network_in, RequestMsg) {
        enqueue(responseIntraChipL2Network_out, ResponseMsg, latency="1") {
          out_msg.Address := address;
          out_msg.PhysicalAddress := in_msg.PhysicalAddress;
          out_msg.Type := CoherenceResponseType:ACK;
          out_msg.Sender := machineID;
          out_msg.Destination.add(in_msg.Requestor);
          out_msg.MessageSize := MessageSizeType:Response_Control;
          // count all L1s except requestor
          out_msg.AckCount := numberOfL1CachePerChip() - 1;
          APPEND_TRANSITION_COMMENT(" Total L1s: ");
          APPEND_TRANSITION_COMMENT(numberOfL1CachePerChip());
          APPEND_TRANSITION_COMMENT(" Total ACKS: ");
          APPEND_TRANSITION_COMMENT(out_msg.AckCount);
          APPEND_TRANSITION_COMMENT(" ");
          APPEND_TRANSITION_COMMENT(in_msg.PhysicalAddress);
        }
      }
    }
  }

  // special INV used when we receive an escape action request. Sharers cannot NACK this invalidate.
  action(fwm_sendFwdInvEscapeToSharersMinusRequestor, "fwme", desc="invalidate sharers for request, requestor is sharer") {
    peek(L1RequestIntraChipL2Network_in, RequestMsg) {
      enqueue(L1RequestIntraChipL2Network_out, RequestMsg, latency="1") {
        out_msg.Address := address;
        out_msg.PhysicalAddress := in_msg.PhysicalAddress;
        out_msg.Type := CoherenceRequestType:INV_ESCAPE;
        out_msg.Requestor := in_msg.Requestor;
        out_msg.Destination := L2cacheMemory[address].Sharers;
        out_msg.Destination.remove(in_msg.Requestor);
        out_msg.MessageSize := MessageSizeType:Request_Control;
        //also pass along timestamp
        out_msg.Timestamp := in_msg.Timestamp;
      }
    }
  }

  action(f_profileRequestor, "prq", desc="Profiles the requestor") {
    peek(L1RequestIntraChipL2Network_in, RequestMsg) {
      APPEND_TRANSITION_COMMENT(" requestor: ");
      APPEND_TRANSITION_COMMENT(in_msg.Requestor);
    }
  }

  // marks the L2 block as transactional if request was transactional
  action(f_markBlockTransIfTrans, "\mbt", desc="Mark an L2 block as transactional") {
    peek(L1unblockNetwork_in, ResponseMsg) {
      if(in_msg.Transactional == true){
        L2cacheMemory[address].Trans := true;
      }
    }    
  }

  action(q_profileOverflow, "po", desc="profile the overflowed block"){
    profileOverflow(address, machineID);
  }

  action(p_profileRequest, "pcc", desc="Profile request msg") {
    peek(L1RequestIntraChipL2Network_in, RequestMsg) {
      APPEND_TRANSITION_COMMENT(" request: Timestamp: ");
      APPEND_TRANSITION_COMMENT(in_msg.Timestamp);
      APPEND_TRANSITION_COMMENT(" Requestor: ");
      APPEND_TRANSITION_COMMENT(in_msg.Requestor);
      APPEND_TRANSITION_COMMENT(" Dest: ");
      APPEND_TRANSITION_COMMENT(in_msg.Destination);
      APPEND_TRANSITION_COMMENT(" PA: ");
      APPEND_TRANSITION_COMMENT(in_msg.PhysicalAddress);
      APPEND_TRANSITION_COMMENT(" Type: ");
      APPEND_TRANSITION_COMMENT(in_msg.Type);
      APPEND_TRANSITION_COMMENT(" Mode: ");
      APPEND_TRANSITION_COMMENT(in_msg.AccessMode);
      APPEND_TRANSITION_COMMENT(" PF: ");
      APPEND_TRANSITION_COMMENT(in_msg.Prefetch);
    }
  }

  //********************************END***************************
 
  action(d_sendDataToRequestor, "d", desc="Send data from cache to reqeustor") {
    peek(L1RequestIntraChipL2Network_in, RequestMsg) {
      enqueue(responseIntraChipL2Network_out, ResponseMsg, latency="L2_RESPONSE_LATENCY") {
        out_msg.Address := address;
        out_msg.PhysicalAddress := in_msg.PhysicalAddress;
        out_msg.Type := CoherenceResponseType:L2_DATA;
        out_msg.Sender := machineID;
        out_msg.Destination.add(in_msg.Requestor);
        out_msg.DataBlk := getL2CacheEntry(address).DataBlk;
        out_msg.Dirty := getL2CacheEntry(address).Dirty;
        out_msg.MessageSize := MessageSizeType:Response_Data;
 
        out_msg.AckCount := 0 - getL2CacheEntry(address).Sharers.count();
        if (getL2CacheEntry(address).Sharers.isElement(in_msg.Requestor)) {
          out_msg.AckCount := out_msg.AckCount + 1;
        }
        APPEND_TRANSITION_COMMENT(" AckCount: ");
        APPEND_TRANSITION_COMMENT(out_msg.AckCount);
      }
    }
  }

  // use DATA instead of L2_DATA because L1 doesn't need to wait for acks from L1 filters in this case
  action(ds_sendSharedDataToRequestor, "ds", desc="Send data from cache to reqeustor") {
    peek(L1RequestIntraChipL2Network_in, RequestMsg) {
      enqueue(responseIntraChipL2Network_out, ResponseMsg, latency="L2_RESPONSE_LATENCY") {
        out_msg.Address := address;
        out_msg.Type := CoherenceResponseType:L2_DATA;
        out_msg.Sender := machineID;
        out_msg.Destination.add(in_msg.Requestor);
        out_msg.DataBlk := getL2CacheEntry(address).DataBlk;
        out_msg.Dirty := getL2CacheEntry(address).Dirty;
        out_msg.MessageSize := MessageSizeType:Response_Data;
        // no ACKS needed because no possible conflicts
        out_msg.AckCount := 0;
      }
    }
  }

  action(f_sendInvToSharers, "fsi", desc="invalidate sharers for L2 replacement") {
    enqueue(L1RequestIntraChipL2Network_out, RequestMsg, latency="L2_TAG_LATENCY") {
      out_msg.Address := address;
      out_msg.Type := CoherenceRequestType:REPLACE;
      out_msg.Requestor := machineID;
      out_msg.Destination := L2cacheMemory[address].Sharers;
      out_msg.MessageSize := MessageSizeType:Request_Control;
    }
  }

  action(fwm_sendFwdInvToSharersMinusRequestor, "fwm", desc="invalidate sharers for request, requestor is sharer") {
    peek(L1RequestIntraChipL2Network_in, RequestMsg) {
      enqueue(L1RequestIntraChipL2Network_out, RequestMsg, latency="L2_TAG_LATENCY") {
        out_msg.Address := address;
        out_msg.PhysicalAddress := in_msg.PhysicalAddress;
        out_msg.Type := CoherenceRequestType:INV;
        out_msg.Requestor := in_msg.Requestor;
        out_msg.Destination := L2cacheMemory[address].Sharers;
        out_msg.Destination.remove(in_msg.Requestor);
        out_msg.MessageSize := MessageSizeType:Request_Control;
        //also pass along timestamp
        out_msg.Timestamp := in_msg.Timestamp;
        APPEND_TRANSITION_COMMENT(" Sharers: ");
        APPEND_TRANSITION_COMMENT(L2cacheMemory[address].Sharers);
      }
    }
  }

  // OTHER ACTIONS
  action(i_allocateTBE, "i", desc="Allocate TBE for internal/external request(isPrefetch=0, number of invalidates=0)") {
    check_allocate(L2_TBEs);
    L2_TBEs.allocate(address);
    L2_TBEs[address].L1_GetS_IDs.clear();
    L2_TBEs[address].DataBlk := getL2CacheEntry(address).DataBlk;
    L2_TBEs[address].Dirty := getL2CacheEntry(address).Dirty;
    L2_TBEs[address].pendingAcks := getL2CacheEntry(address).Sharers.count();
  }

  action(i_setTBEPhysicalAddress, "ia", desc="Sets the physical address field of the TBE"){
    peek(L1RequestIntraChipL2Network_in, RequestMsg) {
      L2_TBEs[address].PhysicalAddress := in_msg.PhysicalAddress;
    }
  }

  action(s_deallocateTBE, "s", desc="Deallocate external TBE") {
    L2_TBEs.deallocate(address);
  }

  action(jj_popL1RequestQueue, "\j", desc="Pop incoming L1 request queue") {
    profileMsgDelay(0, L1RequestIntraChipL2Network_in.dequeue_getDelayCycles());
  }

  action(k_popUnblockQueue, "k", desc="Pop incoming unblock queue") {
    profileMsgDelay(0, L1unblockNetwork_in.dequeue_getDelayCycles());
  }


  action(o_popIncomingResponseQueue, "o", desc="Pop Incoming Response queue") {
    profileMsgDelay(3, responseIntraChipL2Network_in.dequeue_getDelayCycles());
  }

  
  action(m_writeDataToCache, "m", desc="Write data from response queue to cache") {
    peek(responseIntraChipL2Network_in, ResponseMsg) {
      getL2CacheEntry(address).DataBlk := in_msg.DataBlk;
      getL2CacheEntry(address).Dirty := in_msg.Dirty;
      // reset the L2 miss bit
      getL2CacheEntry(address).L2Miss := false;
    }
  }

  // Sets the L2Miss bit in the L2 entry - indicates data was sourced from memory
  action(m_markL2MissBit, "mi", desc="Set the entry's L2 Miss bit") {
    getL2CacheEntry(address).L2Miss := true;
  }

  action(m_copyNackersIntoSharers, "mn", desc="Copy the NACKers list into our sharers list") {
    peek(L1unblockNetwork_in,  ResponseMsg) {
      assert(in_msg.Nackers.count() > 0);
      getL2CacheEntry(address).Sharers.clear();
      // only need to copy into sharers list if we are in special state of "multicast" filter checks
      if(getL2CacheEntry(address).L2Miss == true){
        getL2CacheEntry(address).Sharers := in_msg.Nackers;
        APPEND_TRANSITION_COMMENT(" Unblocker: ");
        APPEND_TRANSITION_COMMENT(in_msg.Sender);
        APPEND_TRANSITION_COMMENT(" Nackers: ");
        APPEND_TRANSITION_COMMENT(getL2CacheEntry(address).Sharers);
      }
    }
  }

  action(mr_writeDataToCacheFromRequest, "mr", desc="Write data from response queue to cache") {
    peek(L1RequestIntraChipL2Network_in, RequestMsg) {
      getL2CacheEntry(address).DataBlk := in_msg.DataBlk;
      getL2CacheEntry(address).Dirty := in_msg.Dirty;
      // reset the L2 miss bit
      getL2CacheEntry(address).L2Miss := false;
    }
  }

  action(q_updateAck, "q", desc="update pending ack count") {
    peek(responseIntraChipL2Network_in, ResponseMsg) {
      L2_TBEs[address].pendingAcks := L2_TBEs[address].pendingAcks - in_msg.AckCount;
      APPEND_TRANSITION_COMMENT(in_msg.AckCount);
      APPEND_TRANSITION_COMMENT(" p: ");
      APPEND_TRANSITION_COMMENT(L2_TBEs[address].pendingAcks);
    }
  }

  // For transactional memory. If received NACK instead of ACK
  action(q_updateNack, "qn", desc="update pending ack count") {
    peek(responseIntraChipL2Network_in, ResponseMsg) {
      // set flag indicating we have seen NACK
      L2_TBEs[address].nack := true;
      L2_TBEs[address].pendingAcks := L2_TBEs[address].pendingAcks - in_msg.AckCount;
      APPEND_TRANSITION_COMMENT(in_msg.AckCount);
      APPEND_TRANSITION_COMMENT(" p: ");
      APPEND_TRANSITION_COMMENT(L2_TBEs[address].pendingAcks);
    }
  }

  action(qq_writeDataToTBE, "\qq", desc="Write data from response queue to TBE") {
    peek(responseIntraChipL2Network_in, ResponseMsg) {
      L2_TBEs[address].DataBlk := in_msg.DataBlk;
      L2_TBEs[address].Dirty := in_msg.Dirty;
    }
  }

  
  action(z_stall, "z", desc="Stall") {
  }
  

  action(ss_recordGetSL1ID, "\s", desc="Record L1 GetS for load response") {
    peek(L1RequestIntraChipL2Network_in, RequestMsg) {
      L2_TBEs[address].L1_GetS_IDs.add(in_msg.Requestor);
    }
  }
  
  action(xx_recordGetXL1ID, "\x", desc="Record L1 GetX for store response") {
    peek(L1RequestIntraChipL2Network_in, RequestMsg) {
      L2_TBEs[address].L1_GetX_ID := in_msg.Requestor;
    }
  }

  action(set_setMRU, "\set", desc="set the MRU entry") {
    L2cacheMemory.setMRU(address);
  }

  action(qq_allocateL2CacheBlock, "\q", desc="Set L2 cache tag equal to tag of block B.") {
    if (L2cacheMemory.isTagPresent(address) == false) {
      L2cacheMemory.allocate(address);
    }
  }

  action(rr_deallocateL2CacheBlock, "\r", desc="Deallocate L2 cache block.  Sets the cache to not present, allowing a replacement in parallel with a fetch.") {
    L2cacheMemory.deallocate(address);
  }

  action(t_sendWBAck, "t", desc="Send writeback ACK") {
    peek(L1RequestIntraChipL2Network_in, RequestMsg) {
      enqueue(responseIntraChipL2Network_out, ResponseMsg, latency="L2_TAG_LATENCY") {
        out_msg.Address := address;
        out_msg.Type := CoherenceResponseType:WB_ACK;
        out_msg.Sender := machineID;
        out_msg.Destination.add(in_msg.Requestor);
        out_msg.MessageSize := MessageSizeType:Response_Control;
      }
    }
  }

  action(ts_sendInvAckToUpgrader, "ts", desc="Send ACK to upgrader") {
    peek(L1RequestIntraChipL2Network_in, RequestMsg) {
      enqueue(responseIntraChipL2Network_out, ResponseMsg, latency="L2_TAG_LATENCY") {
        out_msg.Address := address;
        out_msg.PhysicalAddress := in_msg.PhysicalAddress;
        out_msg.Type := CoherenceResponseType:ACK;
        out_msg.Sender := machineID;
        out_msg.Destination.add(in_msg.Requestor);
        out_msg.MessageSize := MessageSizeType:Response_Control;
        // upgrader doesn't get ack from itself, hence the + 1
        out_msg.AckCount := 0 - getL2CacheEntry(address).Sharers.count() + 1;
        APPEND_TRANSITION_COMMENT(" ");
        APPEND_TRANSITION_COMMENT(in_msg.PhysicalAddress);
      }
    }
  }

  // same as above, but send NACK instead of ACK
  action(ts_sendInvNackToUpgrader, "tsn", desc="Send NACK to upgrader") {
    peek(L1RequestIntraChipL2Network_in, RequestMsg) {
      enqueue(responseIntraChipL2Network_out, ResponseMsg, latency="L2_TAG_LATENCY") {
        out_msg.Address := address;
        out_msg.Type := CoherenceResponseType:NACK;
        out_msg.Sender := machineID;
        out_msg.Destination.add(in_msg.Requestor);
        out_msg.MessageSize := MessageSizeType:Response_Control;
        // upgrader doesn't get ack from itself, hence the + 1
        out_msg.AckCount := 0 - getL2CacheEntry(address).Sharers.count() + 1;
      }
    }
  }

  action(uu_profileMiss, "\u", desc="Profile the demand miss") {
    peek(L1RequestIntraChipL2Network_in, RequestMsg) {
      profile_L2Cache_miss(convertToGenericType(in_msg.Type), in_msg.AccessMode, MessageSizeTypeToInt(in_msg.MessageSize), in_msg.Prefetch, L1CacheMachIDToProcessorNum(in_msg.Requestor));
    }
  }

  action(ww_profileMissNoDir, "\w", desc="Profile this transition at the L2 because Dir won't see the request") {
    peek(L1RequestIntraChipL2Network_in, RequestMsg) {
      // profile_request(in_msg.L1CacheStateStr, getStateStr(address), "NA", getCoherenceRequestTypeStr(in_msg.Type));
    }
  }



  action(nn_addSharer, "\n", desc="Add L1 sharer to list") {
    peek(L1RequestIntraChipL2Network_in, RequestMsg) {
      addSharer(address, in_msg.Requestor);
      APPEND_TRANSITION_COMMENT( getL2CacheEntry(address).Sharers );
    }    
  }

  action(nnu_addSharerFromUnblock, "\nu", desc="Add L1 sharer to list") {
    peek(L1unblockNetwork_in, ResponseMsg) {
      addSharer(address, in_msg.Sender);
      if (in_msg.RemoveLastOwnerFromDir == true) { 
        // We do this to solve some races with PUTX
        APPEND_TRANSITION_COMMENT("Last owner removed, it was ");
        APPEND_TRANSITION_COMMENT(in_msg.LastOwnerID);
        L2cacheMemory[address].Sharers.remove(in_msg.LastOwnerID);
        assert(in_msg.LastOwnerID == L2cacheMemory[address].Exclusive);
      }
    }    
  }


  action(kk_removeRequestSharer, "\k", desc="Remove L1 Request sharer from list") {
    peek(L1RequestIntraChipL2Network_in, RequestMsg) {
      L2cacheMemory[address].Sharers.remove(in_msg.Requestor);
    }    
  }

  action(ll_clearSharers, "\l", desc="Remove all L1 sharers from list") {
      L2cacheMemory[address].Sharers.clear();
  }

  action(mmu_markExclusiveFromUnblock, "\mu", desc="set the exclusive owner") {
    peek(L1unblockNetwork_in, ResponseMsg) {
      if (in_msg.RemoveLastOwnerFromDir == true) { 
        // We do this to solve some races with PUTX
        APPEND_TRANSITION_COMMENT(" Last owner removed, it was ");
        APPEND_TRANSITION_COMMENT(in_msg.LastOwnerID);
        assert(in_msg.LastOwnerID == L2cacheMemory[address].Exclusive);
      }
      L2cacheMemory[address].Sharers.clear();
      L2cacheMemory[address].Exclusive := in_msg.Sender;
      addSharer(address, in_msg.Sender);
    }    
  }

  action(zz_recycleL1RequestQueue, "zz", desc="recycle L1 request queue") {
    peek(L1RequestIntraChipL2Network_in, RequestMsg) {
      if (in_msg.Type == CoherenceRequestType:PUTX || in_msg.Type == CoherenceRequestType:PUTS) {
        if (L2cacheMemory.isTagPresent(in_msg.Address)) {
          getL2CacheEntry(in_msg.Address).L1PutsPending := getL2CacheEntry(in_msg.Address).L1PutsPending  + 1;
          DEBUG_EXPR("RECYCLE PutSPending ");
          DEBUG_EXPR(getL2CacheEntry(in_msg.Address).L1PutsPending);
          DEBUG_EXPR(in_msg.Type);
          DEBUG_EXPR(in_msg.Requestor);
        }
      }
    }
    L1RequestIntraChipL2Network_in.recycle();
  }

  //*****************************************************
  // TRANSITIONS
  //*****************************************************

  /* Recycle while waiting for PUT  */
  transition({PB_MT, PB_MT_IB, PB_SS}, {L1_GETS, L1_GET_INSTR, L1_GETX, L1_UPGRADE, L1_GETS_ESCAPE, L1_GETX_ESCAPE, L1_GET_INSTR_ESCAPE, L2_Replacement, L2_Replacement_clean, L2_Replacement_XACT, L2_Replacement_clean_XACT}) {
    f_profileRequestor;
    zz_recycleL1RequestQueue;
  }

  transition({IM, IS, ISS, SS_MB, M_MB, ISS_MB, IS_SSB, MT_MB, MT_IIB, MT_IB, MT_SB, M_SSB, SS_SSB}, 
	     {L2_Replacement, L2_Replacement_clean, L2_Replacement_XACT, L2_Replacement_clean_XACT}) {
    zz_recycleL1RequestQueue;
  }

  transition({SS_MB, M_MB, ISS_MB, IS_SSB, MT_MB, MT_IIB, MT_IB, MT_SB, M_SSB, SS_SSB}, 
	     {L1_GETS, L1_GET_INSTR, L1_GETX, L1_UPGRADE, L1_GETS_ESCAPE, L1_GETX_ESCAPE, L1_GET_INSTR_ESCAPE}) {
    f_profileRequestor;
    zz_recycleL1RequestQueue;
  }

  transition({NP, SS, M, M_I, MT_I, MCT_I, I_I, S_I, ISS, IS, IM, /*SS_MB,*/ SS_SSB, /* MT_MB, M_MB, ISS_MB,*/ IS_SSB, M_SSB, /*MT_IIB, */MT_IB/*, MT_SB*/}, {L1_PUTX,L1_PUTS}) {
    f_profileRequestor;
    zz_recycleL1RequestQueue;
  }

  // occurs when L2 replacement raced with L1 replacement, and L2 finished its replacement first
  transition({NP, M_I, MCT_I, I_I, S_I, IS, ISS, IM, SS, M, MT, IS_SSB, MT_IB, M_SSB, SS_SSB}, {L1_PUTX_old, L1_PUTS_old}){
    f_profileRequestor;
    jj_popL1RequestQueue;
  }

  // PUT from current (last) exclusive owner, that was replacing the line when it received Fwd req
  transition(MT_I, {L1_PUTX_old, L1_PUTS_old}) { 
    f_profileRequestor;
    jj_popL1RequestQueue;
  }

  transition({SS, M, MT}, {L1_PUT_PENDING}) { // L1_PUT_ msg pending for the block, don't accept new requests until PUT is processed */
    f_profileRequestor;
    zz_recycleL1RequestQueue;
  }

  //===============================================
  // BASE STATE - I

  // Transitions from I (Idle)

  // When L2 doesn't have block, need to send broadcasst to all L1s to check appropriate filter(s)
  transition(NP, L1_GETS,  ISS) {
    p_profileRequest;
    f_profileRequestor;
    qq_allocateL2CacheBlock;
    ll_clearSharers;
    // will mark as exclusive when we get unblocked with success
    //nn_addSharer;
    i_allocateTBE;
    i_setTBEPhysicalAddress;
    ss_recordGetSL1ID;
    a_issueFetchToMemory;
    // for correctness we need to query both read + write filters
    a_checkL1ReadWriteFiltersExceptRequestor;
    uu_profileMiss;
    jj_popL1RequestQueue;
  }

  // no need to check filters, send accumulated ACK to requestor
  transition(NP, L1_GETS_ESCAPE, ISS) {
    p_profileRequest;
    f_profileRequestor;
    qq_allocateL2CacheBlock;
    ll_clearSharers;
    // will mark as exclusive when we get unblocked with success
    //nn_addSharer;
    i_allocateTBE;
    i_setTBEPhysicalAddress;
    ss_recordGetSL1ID;
    a_issueFetchToMemory;
    // send accumulated ACK
    f_sendAccumulatedAckToRequestor;
    uu_profileMiss;
    jj_popL1RequestQueue;
  }

  transition(NP, L1_GET_INSTR, IS) {
    p_profileRequest;
    f_profileRequestor;
    qq_allocateL2CacheBlock;
    ll_clearSharers;
    nn_addSharer;
    i_allocateTBE;
    i_setTBEPhysicalAddress;
    ss_recordGetSL1ID;
    a_issueFetchToMemory;
    // for correctness query the read + write filters
    a_checkL1ReadWriteFiltersExceptRequestor;
    uu_profileMiss;
    jj_popL1RequestQueue;
  }

  // no need to query filters, send accumluated ACK to requestor
  transition(NP, L1_GET_INSTR_ESCAPE, IS) {
    p_profileRequest;
    f_profileRequestor;
    qq_allocateL2CacheBlock;
    ll_clearSharers;
    nn_addSharer;
    i_allocateTBE;
    i_setTBEPhysicalAddress;
    ss_recordGetSL1ID;
    a_issueFetchToMemory;
    // send accumulated ACK
    f_sendAccumulatedAckToRequestor;
    uu_profileMiss;
    jj_popL1RequestQueue;
  }

  transition(NP, L1_GETX, IM) {  
    p_profileRequest;
    f_profileRequestor;
    qq_allocateL2CacheBlock;
    ll_clearSharers;
    // nn_addSharer;
    i_allocateTBE;
    i_setTBEPhysicalAddress;
    xx_recordGetXL1ID;
    a_issueFetchToMemory;
    // also query the L1 write and read filters
    a_checkL1ReadWriteFiltersExceptRequestor;
    uu_profileMiss;
    jj_popL1RequestQueue;
  }

  // don't check filters
  transition(NP, L1_GETX_ESCAPE, IM) {  
    p_profileRequest;
    f_profileRequestor;
    qq_allocateL2CacheBlock;
    ll_clearSharers;
    // nn_addSharer;
    i_allocateTBE;
    i_setTBEPhysicalAddress;
    xx_recordGetXL1ID;
    a_issueFetchToMemory;
    // send accumulated ACK to requestor
    f_sendAccumulatedAckToRequestor;
    uu_profileMiss;
    jj_popL1RequestQueue;
  }


  // transitions from IS/IM

  // force L1s to respond success or failure
  transition(ISS, Mem_Data, ISS_MB){
    m_writeDataToCache;
    m_markL2MissBit;
    // send exclusive data but force L1 to wait for filter responses
    f_sendExclusiveDataToGetSRequestor;
    s_deallocateTBE;
    o_popIncomingResponseQueue;
  }

  transition(IS, Mem_Data, IS_SSB){
    m_writeDataToCache;
    m_markL2MissBit;
    // send data but force L1 to wait for filter responses
    f_sendDataToGetSRequestor;
    s_deallocateTBE;
    o_popIncomingResponseQueue;
  }

  transition(IM, Mem_Data, ISS_MB){
    m_writeDataToCache;
    m_markL2MissBit;
    // send data but force L1 to wait for filter responses
    f_sendDataToGetXRequestor;
    s_deallocateTBE;
    o_popIncomingResponseQueue;
  }
    
  // disallow grouping of requestors. There is a correctness problem if we check the wrong
  //  filters as indicated by the original requestor.
  transition({IS, ISS}, {L1_GETX, L1_GETS, L1_GET_INSTR, L1_GETX_ESCAPE, L1_GETS_ESCAPE, L1_GET_INSTR_ESCAPE}) {
    f_profileRequestor;
    zz_recycleL1RequestQueue;
  }

  transition(IM, {L1_GETX, L1_GETS, L1_GET_INSTR, L1_GETX_ESCAPE, L1_GETS_ESCAPE, L1_GET_INSTR_ESCAPE}) {
    f_profileRequestor;
    zz_recycleL1RequestQueue;
  }

  // transitions from SS
  transition(SS, {L1_GETS, L1_GET_INSTR, L1_GETS_ESCAPE, L1_GET_INSTR_ESCAPE}, SS_SSB) {
    p_profileRequest;
    f_profileRequestor;
    ds_sendSharedDataToRequestor;
    nn_addSharer;
    uu_profileMiss;
    set_setMRU;
    jj_popL1RequestQueue;
  }

  // For isolation the L1 filters might return NACKs to the requestor
  transition(SS, L1_GETX, SS_MB) {
    p_profileRequest;
    f_profileRequestor;
    d_sendDataToRequestor;
    fwm_sendFwdInvToSharersMinusRequestor;
    uu_profileMiss;
    set_setMRU;
    jj_popL1RequestQueue;
  }

  // send special INV to sharers - they have to invalidate
  transition(SS, L1_GETX_ESCAPE, SS_MB) {
    p_profileRequest;
    f_profileRequestor;
    d_sendDataToRequestor;
    fwm_sendFwdInvEscapeToSharersMinusRequestor;
    uu_profileMiss;
    set_setMRU;
    jj_popL1RequestQueue;
  }

  // For isolation the L1 filters might return NACKs to the requestor
  transition(SS, L1_UPGRADE, SS_MB) {
    f_profileRequestor;
    fwm_sendFwdInvToSharersMinusRequestor;
    ts_sendInvAckToUpgrader;
    uu_profileMiss;
    set_setMRU;
    jj_popL1RequestQueue;
  }
  
  transition(SS, L2_Replacement_clean, I_I) {
    i_allocateTBE;
    f_sendInvToSharers;
    rr_deallocateL2CacheBlock;
  }

  transition(SS, L2_Replacement_clean_XACT, I_I) {
    q_profileOverflow;
    i_allocateTBE;
    f_sendInvToSharers;
    rr_deallocateL2CacheBlock;
  }

  transition(SS, L2_Replacement, S_I) {
    i_allocateTBE;
    f_sendInvToSharers;
    rr_deallocateL2CacheBlock;
  }

  transition(SS, L2_Replacement_XACT, S_I) {
    q_profileOverflow;
    i_allocateTBE;
    f_sendInvToSharers;
    rr_deallocateL2CacheBlock;
  }

  // Transitions from M

  // send data, but force L1 to wait for filter responses
  transition(M, L1_GETS, M_MB) {
    p_profileRequest;
    f_profileRequestor;
    f_sendExclusiveDataToRequestor;
    // selective filter checks, but need to check both read+write in case nackers put NP block into M state
    a_checkNackerL1ReadWriteFiltersExceptRequestor;    
    uu_profileMiss;
    set_setMRU;
    jj_popL1RequestQueue;
  }

  // don't care about filters
  transition(M, L1_GETS_ESCAPE, M_MB) {
    p_profileRequest;
    f_profileRequestor;
    f_sendExclusiveDataToRequestor;
    uu_profileMiss;
    set_setMRU;
    jj_popL1RequestQueue;
  }

  transition(M, L1_GET_INSTR, M_SSB) {
    p_profileRequest;
    f_profileRequestor;
    f_sendDataToRequestor;
    // NEW - selective filter checks, but need to check both read+write in case nackers put NP block into M state
    a_checkNackerL1ReadWriteFiltersExceptRequestor;    
    // This should always be _after_ f_sendDataToRequestor and a_checkNackerL1WriteFiltersExceptRequestor, since they
    //   explicitly look at the sharers list!
    nn_addSharer;
    uu_profileMiss;
    set_setMRU;
    jj_popL1RequestQueue;
  }

  // don't care about filters
  transition(M, L1_GET_INSTR_ESCAPE, M_SSB) {
    p_profileRequest;
    f_profileRequestor;
    f_sendDataToRequestor;
    nn_addSharer;
    uu_profileMiss;
    set_setMRU;
    jj_popL1RequestQueue;
  }

  transition(M, L1_GETX, M_MB) {
    p_profileRequest;
    f_profileRequestor;
    f_sendDataToRequestor;
    // selective filter checks
    a_checkNackerL1ReadWriteFiltersExceptRequestor;
    // issue filter checks
    //a_checkL1ReadWriteFiltersExceptRequestor;
    uu_profileMiss;
    set_setMRU;
    jj_popL1RequestQueue;
  }

  // don't care about filters
  transition(M, L1_GETX_ESCAPE, M_MB) {
    p_profileRequest;
    f_profileRequestor;
    f_sendDataToRequestor;
    uu_profileMiss;
    set_setMRU;
    jj_popL1RequestQueue;
  }

  transition(M, L2_Replacement, M_I) {
    i_allocateTBE;
    c_exclusiveReplacement;
    rr_deallocateL2CacheBlock;
  }

  transition(M, L2_Replacement_clean, M_I) {
    rr_deallocateL2CacheBlock;
  }

  transition(M, L2_Replacement_XACT, M_I) {
    q_profileOverflow;
    i_allocateTBE;
    c_exclusiveReplacement;
    rr_deallocateL2CacheBlock;
  }

  transition(M, L2_Replacement_clean_XACT, M_I) {
    q_profileOverflow;
    rr_deallocateL2CacheBlock;
  }


  // transitions from MT
  transition(MT, {L1_GETX, L1_GETX_ESCAPE}, MT_MB) {
    p_profileRequest;
    f_profileRequestor;
    b_forwardRequestToExclusive;
    uu_profileMiss;
    set_setMRU;
    jj_popL1RequestQueue;
  }


  transition(MT, {L1_GETS, L1_GET_INSTR, L1_GETS_ESCAPE, L1_GET_INSTR_ESCAPE}, MT_IIB) {
    p_profileRequest;
    f_profileRequestor;
    b_forwardRequestToExclusive;
    uu_profileMiss;
    set_setMRU;
    jj_popL1RequestQueue;
  }

  transition(MT, L2_Replacement, MT_I) {
    i_allocateTBE;
    f_sendInvToSharers;
    rr_deallocateL2CacheBlock;
  }

  transition(MT, L2_Replacement_clean, MCT_I) {
    i_allocateTBE;
    f_sendInvToSharers;
    rr_deallocateL2CacheBlock;
  }

  transition(MT, L2_Replacement_XACT, MT_I) {
    q_profileOverflow;
    i_allocateTBE;
    f_sendInvToSharers;
    rr_deallocateL2CacheBlock;
  }

  transition(MT, L2_Replacement_clean_XACT, MCT_I) {
    q_profileOverflow;
    i_allocateTBE;
    f_sendInvToSharers;
    rr_deallocateL2CacheBlock;
  }

  transition(MT, L1_PUTX, M) {
    f_profileRequestor;
    // this doesn't affect exlusive ptr
    ll_clearSharers;
    mr_writeDataToCacheFromRequest;
    t_sendWBAck;
    jj_popL1RequestQueue;
  }

  // This is for the case of transactional read line in E state being replaced from L1. We need to maintain isolation on this 
  //        in the event of a future transactional store from another proc, so we maintain this transactional sharer on the list
  transition(MT, L1_PUTS, SS) {
    f_profileRequestor;
    ll_clearSharers;
    // maintain transactional read isolation
    nn_addSharer;
    mr_writeDataToCacheFromRequest;
    t_sendWBAck;
    jj_popL1RequestQueue;
  }
 
  // transitions from blocking states
  transition(SS_MB, Unblock_Cancel, SS) {
    k_popUnblockQueue;
  }

  transition(M_SSB, Unblock_Cancel, M) {
    ll_clearSharers;
    // copy NACKers list from unblock message to our sharers list
    m_copyNackersIntoSharers;
    k_popUnblockQueue;
  }

  transition(MT_MB, Unblock_Cancel, MT) {
    k_popUnblockQueue;
  }

  transition(MT_IB, Unblock_Cancel, MT) {
    k_popUnblockQueue;
  }

  transition(MT_IIB, Unblock_Cancel, MT){
    k_popUnblockQueue;
  }

  // L2 just got the data from memory, but we have Nackers. We can let nacked block reside in M, but GETS request needs to check read+write
  //   signatures to avoid atomicity violations.
  transition({ISS_MB, IS_SSB}, Unblock_Cancel, M){
    //rr_deallocateL2CacheBlock;
    // copy NACKers list from unblock message to our sharers list
    m_copyNackersIntoSharers;
    k_popUnblockQueue;
  }

  transition(M_MB, Unblock_Cancel, M) {
    // copy NACKers list from unblock message to our sharers list
    m_copyNackersIntoSharers;
    k_popUnblockQueue;
  }

  transition(SS_MB, Exclusive_Unblock, MT) {
    // update actual directory
    mmu_markExclusiveFromUnblock;
    // mark block as trans if needed
    f_markBlockTransIfTrans;
    k_popUnblockQueue;
  }

  // PUT from next exclusive surpassed its own ExclusiveUnblock  
  // Perceived as PUTX_old because the directory is outdated
  transition(SS_MB, {L1_PUTX_old, L1_PUTS_old}) { 
    f_profileRequestor;
    zz_recycleL1RequestQueue;
  }

  // PUT from current (old) exclusive, can't do anything with it in this state 
  // Don't know whether exclusive was replacing or not, so wait to see what Unblock says 
  transition(SS_MB, {L1_PUTX, L1_PUTS}) { 
    f_profileRequestor;
    zz_recycleL1RequestQueue;
  }

  // Next exclusive informs that last owner was replacing the line when it received Fwd req
  // Thus, expect a PUTX_old from previous owner
  transition(SS_MB, Exclusive_Unblock_WaitPUTold, PB_MT) { 
    // update actual directory
    mmu_markExclusiveFromUnblock;
    // mark block as trans if needed
    f_markBlockTransIfTrans;
    k_popUnblockQueue;
  }

  transition(PB_MT, {L1_PUTX_old, L1_PUTS_old}, MT) { // OK, PUT_old received, go to MT
    f_profileRequestor;
    jj_popL1RequestQueue;
  }
  
  // PUT from current (next) exclusive, so recycle
  // Expecting PUT_old, won't take in new PUT until previous PUT arrives
  transition(PB_MT, {L1_PUTX, L1_PUTS}) { 
    f_profileRequestor;
    zz_recycleL1RequestQueue;
  }

  // L2 blocks on GETS requests in SS state
  transition(SS_SSB, Unblock, SS) {
    // mark block as trans if needed
    f_markBlockTransIfTrans;
    k_popUnblockQueue;
  } 

  transition({M_SSB, IS_SSB}, Unblock, SS) {
    // we already added the sharer when we received original request
    // mark block as trans if needed
    f_markBlockTransIfTrans;
    k_popUnblockQueue;
  }

  transition({M_MB, MT_MB, ISS_MB}, Exclusive_Unblock, MT) {
    // update actual directory
    mmu_markExclusiveFromUnblock;
    // mark block as trans if needed
    f_markBlockTransIfTrans;
    k_popUnblockQueue;
  }

  transition({M_MB, MT_MB, ISS_MB}, Exclusive_Unblock_WaitPUTold, PB_MT) {
    // update actual directory
    mmu_markExclusiveFromUnblock;
    // mark block as trans if needed
    f_markBlockTransIfTrans;
    k_popUnblockQueue;
  }

  // PUT from (not yet) next exclusive surpassed its own ExclusiveUnblock 
  // thus became PUTX_old (since directory is not up-to-date)
  transition({M_MB, MT_MB, ISS_MB}, {L1_PUTX_old, L1_PUTS_old}) {
    f_profileRequestor;
    zz_recycleL1RequestQueue;
  }

  // PUT from current (previous) owner: recycle until unblock arrives
  // We don't know whether replacing cache is waiting for WB_Ack or it was replacing when fwd arrived
  transition({M_MB, MT_MB, ISS_MB}, {L1_PUTX, L1_PUTS}) { 
    f_profileRequestor;
    zz_recycleL1RequestQueue;
  }

  // L1 requestor received data from exclusive L1, but writeback data from exclusive L1 hasn't arrived yet
  transition(MT_IIB, Unblock, MT_IB) {
    nnu_addSharerFromUnblock;
    // mark block as trans if needed
    f_markBlockTransIfTrans;
    k_popUnblockQueue;
  }

  // PUT from current (previous) owner: recycle 
  // We don't know whether replacing cache is waiting for WB_Ack or it was replacing when fwd arrived
  transition(MT_IIB, {L1_PUTX, L1_PUTS}) { 
    f_profileRequestor;
    zz_recycleL1RequestQueue;
  }

  transition(MT_IB, {WB_Data, WB_Data_clean}, SS) {
    m_writeDataToCache;
    o_popIncomingResponseQueue;
  }

  // PUT from (not yet) next exclusive, but unblock hasn't arrived yet, so it became PUT_old: recycle
  transition(MT_IIB, {L1_PUTX_old, L1_PUTS_old}) { 
    f_profileRequestor;
    zz_recycleL1RequestQueue;
  }

  transition(MT_IIB, Unblock_WaitPUTold, PB_MT_IB) { // Now arrives Unblock, wait for PUT and WB_Data
    nnu_addSharerFromUnblock;
    // mark block as trans if needed
    f_markBlockTransIfTrans;
    k_popUnblockQueue;
  }
  
  // L1 requestor has not received data from exclusive L1, but we received writeback data from exclusive L1
  transition(MT_IIB, {WB_Data, WB_Data_clean}, MT_SB) {
    m_writeDataToCache;
    o_popIncomingResponseQueue;
  }

  // PUT_old from previous owner, that was replacing when it received Fwd req
  transition(PB_MT_IB, {L1_PUTX_old, L1_PUTS_old}, MT_IB) { // Go to MT_IB, and wait for WB_Data
    f_profileRequestor;
    jj_popL1RequestQueue;
  }

  transition(PB_MT_IB, {L1_PUTX, L1_PUTS}) { // Waiting for PUT_old, don't take new PUT in
    f_profileRequestor;
    zz_recycleL1RequestQueue;
  }

  // WB_data from previous owner, we already received unblock, just wait for PUT_old to go to SS
  transition(PB_MT_IB, {WB_Data, WB_Data_clean}, PB_SS) { // Received Unblock, now arrives WB_Data, wait for PUT
    m_writeDataToCache;
    o_popIncomingResponseQueue;
  }

  transition(PB_SS, {L1_PUTX_old, L1_PUTS_old}, SS) { // Received Unblock and WB_Data, now arrives PUT, go to SS
    f_profileRequestor;
    jj_popL1RequestQueue;
  }

  // PUT from new exclusive owner, while waiting for PUT from previous exclusive owner: recycle
  transition(PB_SS, {L1_PUTX, L1_PUTS}) { 
    f_profileRequestor;
    zz_recycleL1RequestQueue;
  }

  transition(MT_SB, Unblock, SS) {
    nnu_addSharerFromUnblock;
    // mark block as trans if needed
    f_markBlockTransIfTrans;
    k_popUnblockQueue;
  }

  transition(MT_SB, Unblock_WaitPUTold, PB_SS) { // Received WB_Data, now arriving Unblock, wait for PUT
    nnu_addSharerFromUnblock;
    // mark block as trans if needed
    f_markBlockTransIfTrans;
    k_popUnblockQueue;
  }

  // PUT from (not yet) new exclusive owner, before we receive Unblock from it (became PUT_old because directory is not up-to-date)
  transition(MT_SB, {L1_PUTX_old, L1_PUTS_old}) { 
    f_profileRequestor;
    zz_recycleL1RequestQueue;
  }

  // PUT from current (last) exclusive owner, that was replacing the line when it received Fwd req
  transition(MT_SB, {L1_PUTX, L1_PUTS}) { 
    kk_removeRequestSharer; // When Unblock arrives, it'll trigger Unblock, not Unblock_WaitPUTold
    f_profileRequestor;
    jj_popL1RequestQueue;
  }

  // writeback states
  transition({I_I, S_I, MT_I, MCT_I, M_I}, {L1_GETX, L1_UPGRADE, L1_GETS, L1_GET_INSTR, L1_GETX_ESCAPE, L1_GETS_ESCAPE, L1_GET_INSTR_ESCAPE}) {
    f_profileRequestor;
    zz_recycleL1RequestQueue;
  }

  transition(I_I, Ack) {
    q_updateAck;
    o_popIncomingResponseQueue;
  }

  transition(I_I, Ack_all, NP) {
    s_deallocateTBE;
    o_popIncomingResponseQueue;
  }

  transition({MT_I, MCT_I}, WB_Data, M_I) {
    qq_writeDataToTBE;
    ct_exclusiveReplacementFromTBE;
    o_popIncomingResponseQueue;
  }
  
  transition(MCT_I, WB_Data_clean, NP) {
    s_deallocateTBE;
    o_popIncomingResponseQueue;
  }
  
  // L1 never changed Dirty data
  transition(MT_I, Ack_all, M_I) {
    ct_exclusiveReplacementFromTBE;
    o_popIncomingResponseQueue;
  }

  // clean data that L1 exclusive never wrote
  transition(MCT_I, Ack_all, NP) {
    s_deallocateTBE;
    o_popIncomingResponseQueue;
  }

  transition(MT_I, WB_Data_clean, NP) {
    s_deallocateTBE;
    o_popIncomingResponseQueue;
  }

  transition(S_I, Ack) {
    q_updateAck;
    o_popIncomingResponseQueue;
  }

  transition(S_I, Ack_all, M_I) {
    ct_exclusiveReplacementFromTBE;
    o_popIncomingResponseQueue;
  }

  transition(M_I, Mem_Ack, NP) {
    s_deallocateTBE;
    o_popIncomingResponseQueue;
  }
}

